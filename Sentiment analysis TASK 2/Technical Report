Technical Report on LSTM Model Performance for Sentiment Analysis
1. Introduction
Sentiment analysis is a crucial application in Natural Language Processing (NLP) that classifies text into categories such as positive, negative, and neutral. In this study, we utilize a Long Short-Term Memory (LSTM) model to analyze sentiment in text data, leveraging embedding layers and recurrent architectures for improved performance.
2. Data Preprocessing
Before feeding data into the model, we performed several preprocessing steps:
- Tokenization of text using Keras Tokenizer, converting words into indexed values.
- Padding sequences to ensure uniform input dimensions.
- Encoding sentiment labels using LabelBinarizer for binary classification and one-hot encoding for multi-class classification.
- Splitting the dataset into training (70%) and testing (30%) using train_test_split.
3. Model Architecture
The LSTM model is built using TensorFlow and Keras, comprising:
- Embedding Layer: Converts words into dense vectors (word embeddings).
- SpatialDropout1D: Adds regularization to prevent overfitting.
- LSTM Layer: Captures sequential dependencies in textual data.
- Dense Output Layer: Uses Softmax activation for multi-class classification.
The model is compiled using:
- Categorical Crossentropy loss function.
- Adam optimizer for adaptive learning.
- Accuracy metric for performance evaluation.
4. Training and Evaluation
The model was trained over 500 epochs, producing the following results:
- Training Accuracy: 99.77%
- Training Loss: 0.0109
- Test Accuracy: 69.78%
- Test Loss: 1.6447
Findings
- The training accuracy is exceptionally high, indicating strong convergence.
- The test accuracy is significantly lower, suggesting overfitting.
- The loss disparity between training and testing indicates the model memorized patterns instead of generalizing well.
5. Overfitting Analysis & Mitigation
Overfitting occurs when a model performs well on training data but struggles on unseen test data. Possible solutions:
- Increase dropout rate to reduce reliance on specific neurons.
- Reduce the number of LSTM units to prevent excessive model complexity.
- Experiment with batch normalization to stabilize learning.
- Use more diverse datasets to enhance model generalization.
